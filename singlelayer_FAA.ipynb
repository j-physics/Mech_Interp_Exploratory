{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNsyswJ1QLXzSs9mFvT1HQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j-physics/Mech_Interp_Exploratory/blob/main/singlelayer_FAA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnOapspCw2W0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformer-lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries or packages\n",
        "import torch\n",
        "import transformer_lens\n",
        "from transformer_lens import HookedTransformer\n",
        "\n",
        "#check GPU is working right\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {
        "id": "B-5rAOPBxfTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load a small model (e.g., gpt2-small, pythia-70m)\n",
        "model = HookedTransformer.from_pretrained(\"pythia-70m\", device=\"cuda\")\n",
        "\n",
        "#check to see model is loading\n",
        "text = \"Hi, my name is Jessica.\"\n",
        "tokens = model.to_tokens(text)\n",
        "logits = model(tokens)\n",
        "print(f\"Tokens shape: {tokens.shape}\")\n",
        "print(f\"Logits shape: {logits.shape}\")\n"
      ],
      "metadata": {
        "id": "WqJVFHl-zokj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sae_lens"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iqIDefdg7GyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#integrate SAEs from TransformerLens\n",
        "from transformer_lens import HookedTransformer\n",
        "from sae_lens import SAE\n",
        "\n",
        "#Load SAE for a specific layer(s)\n",
        "sae, cfg_dict, sparsity = SAE.from_pretrained_with_cfg_and_sparsity(\n",
        "    release=\"ctigges/pythia-70m-deduped__res-sm_processed\",\n",
        "    sae_id=\"2-res-sm\",\n",
        "    device=\"cuda\"\n",
        ")"
      ],
      "metadata": {
        "id": "BO1-wHCw5cEE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature activation analysis (FAA)\n",
        "\n",
        "#Testing some text out\n",
        "text = \"There is a tiger in the room\"\n",
        "tokens = model.to_tokens(text)\n",
        "\n",
        "#Getting the activations at a single layer\n",
        "_, cache = model.run_with_cache(tokens)\n",
        "layer_acts = cache[\"blocks.2.hook_resid_post\"]\n",
        "\n",
        "#Run through SAE to get feature activations of singlelayer\n",
        "feature_acts = sae.encode(layer_acts)\n",
        "\n",
        "#Seeing which features fired\n",
        "print(f\"Shape: {feature_acts.shape}\") #should be in the format [batch, seq_len, num_feat]\n",
        "print(f\"Non-zero features: {(feature_acts > 0).sum()}\")\n",
        "\n",
        "#Find the top activating features\n",
        "top_features = feature_acts[0, -1].topk(10) #top 10 at the last token\n",
        "print(f\"Top features: {top_features.indices}\")\n",
        "print(f\"Activations: {top_features.values}\")"
      ],
      "metadata": {
        "id": "zBC_g6DxzY-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Looking at each token position now\n",
        "tokens_str = model.to_str_tokens(text)\n",
        "print(\"Tokens:\", tokens_str)\n",
        "\n",
        "# for pos in range(len(tokens_str)):\n",
        "#   top_at_pos = feature_acts[0, pos].topk(5)\n",
        "#   print(f\"\\nToken {pos} ('{tokens_str[pos]}'):\")\n",
        "#   print(f\" Top features: {top_at_pos.indices.tolist()}\")\n",
        "#   #print(f\" Activations: {top_at_pos.values.tolist()}\")\n",
        "\n",
        "#nice format to show what sentence I am analyzing\n",
        "print(\"=\" * 60)\n",
        "print(f\"Analyzing: '{text}'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "#What top features activate for each token\n",
        "for pos, token in enumerate(tokens_str):\n",
        "  top_features = feature_acts[0, pos].topk(3) #top three features\n",
        "  print(f\"\\nPosition {pos}: '{token}'\")\n",
        "  print(f\" Top 3 features: {top_features.indices.tolist()}\")\n",
        "  print(f\" Activations: {[f'{x: .3f}' for x in top_features.values.tolist()]}\")\n"
      ],
      "metadata": {
        "id": "PrA9sMsZzfZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What about for some other sentences? (including the previous test text)\n",
        "test_sentences = {\n",
        "    \"original\": \"The tiger is in the room\",\n",
        "    \"changed_subject\": \"The wolf is in the room\", #changing the subject\n",
        "    \"changed_verb\": \"The tiger jumped in the room\", #change of verb\n",
        "    \"multiple_subjects\": \"The tiger and the wolf are in the room\", #multiple subjects\n",
        "}\n",
        "\n",
        "#collect the activations for each sentence\n",
        "results = {}\n",
        "\n",
        "for name, sentence in test_sentences.items():\n",
        "  tokens = model.to_tokens(sentence)\n",
        "  tokens_str = model.to_str_tokens(sentence)\n",
        "\n",
        "  _, cache = model.run_with_cache(tokens)\n",
        "  layer_activate = cache[\"blocks.2.hook_resid_post\"]\n",
        "  feature_activate = sae.encode(layer_activate)\n",
        "\n",
        "  results[name] = {\n",
        "        'tokens': tokens_str,\n",
        "        'activations': feature_activate[0] #[seq_len, num_features]\n",
        "  }\n",
        "\n",
        "  #Comparing which features appear in ALL variations\n",
        "  #Getting the top features from each\n",
        "  all_top_features = []\n",
        "  for name, data in results.items():\n",
        "    top = data['activations'][-1].topk(10).indices.tolist() #last token\n",
        "    all_top_features.extend(top)\n",
        "\n",
        "  #Find common features\n",
        "  from collections import Counter\n",
        "  feature_counts = Counter(all_top_features)\n",
        "\n",
        "  print(\"=\" * 60)\n",
        "  print(\"Feature Consistency Analysis\")\n",
        "\n",
        "  print(\"Features that consistently activate across variations:\")\n",
        "  print(common_features)\n"
      ],
      "metadata": {
        "id": "JZXYToMv38Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What about for some other sentences? (including the previous test text)\n",
        "test_sentences = {\n",
        "    \"original\": \"The tiger is in the room\",\n",
        "    \"changed_subject\": \"The wolf is in the room\", #changing the subject\n",
        "    \"changed_verb\": \"The tiger jumped in the room\", #change of verb\n",
        "    \"multiple_subjects\": \"The tiger and the wolf are in the room\", #multiple subjects\n",
        "}\n",
        "\n",
        "#collect the activations for each sentence\n",
        "results = {}\n",
        "\n",
        "for name, sentence in test_sentences.items():\n",
        "  tokens = model.to_tokens(sentence)\n",
        "  tokens_str = model.to_str_tokens(sentence)\n",
        "\n",
        "  _, cache = model.run_with_cache(tokens)\n",
        "  layer_activate = cache[\"blocks.2.hook_resid_post\"]\n",
        "  feature_activate = sae.encode(layer_activate)\n",
        "\n",
        "  results[name] = {\n",
        "        'tokens': tokens_str,\n",
        "        'activations': feature_activate[0] #[seq_len, num_features]\n",
        "  }\n",
        "\n",
        "#Comparing which features appear in ALL variations\n",
        "#Getting the top features from each\n",
        "all_top_features = []\n",
        "for name, data in results.items():\n",
        "    top = data['activations'][-1].topk(10).indices.tolist() #last token\n",
        "    all_top_features.extend(top)\n",
        "\n",
        "#Find common features\n",
        "from collections import Counter\n",
        "feature_counts = Counter(all_top_features)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Feature Consistency Analysis\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nFeatures that appear most frequently across variations:\")\n",
        "for feat, count in feature_counts.most_common(10):\n",
        "  print(f\"   Feature {feat}: appears in {count}/{len(test_sentences)} variations\")\n"
      ],
      "metadata": {
        "id": "Z-3Aq4zNStU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's visualize what is happening\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Generate a heatmap of feature activations across tokens\n",
        "activations = feature_acts[0].cpu().detach() # [seq_len, num_featueres]\n",
        "\n",
        "#Just plotting features that activated\n",
        "active_mask = (activations > 0).any(dim=0)\n",
        "active_features = activations[:, active_mask]\n",
        "\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.imshow(active_features.T[:100], aspect='auto', cmap='hot')\n",
        "plt.xlabel('Token Position')\n",
        "plt.ylabel('Feature Index')\n",
        "plt.title('Feature Activations Across Sequence')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K5RyRvgn5KuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}